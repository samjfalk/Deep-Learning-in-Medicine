{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/sfalk/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/sfalk/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/sfalk/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/sfalk/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/sfalk/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/sfalk/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Medicine - Spring 2020\n",
    "### BMSC-GA 4493, BMIN-GA 3007 \n",
    "### Homework 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you need to write mathematical terms, you can type your answeres in a Markdown Cell via LaTex \n",
    "\n",
    "See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\"> here </a>.\n",
    "\n",
    "**Submission instruction**: Upload and Submit your final jupyter notebook with necessary files in <a href='http://newclasses.nyu.edu'>newclasses.nyu.edu</a>. If you use code or script from web, please give a link to the code in your answers. Not providing the reference of the code used will reduce your points!!\n",
    "\n",
    "**Submission deadline: Friday March 24th 2019 (5:00 PM) --> No Extensions!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Convolutional Layer  (Total 20 points)\n",
    "\n",
    "We have a 3x4x4 image (3 channels) and four 3x3x3 convolution kernels as pictured. Bias term for each feature map is also provided. For the questions below, please provide the feature/activation maps requested, please provide the python code that you used to calculate the maps\n",
    "\n",
    "<img src=\"q1_picture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) \n",
    "What will be the dimension of the feature maps after we forward propogate the image using the given convolution kernels for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'w', 'b']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# starter code to load image:x, kernel weights:w and bias:b\n",
    "import numpy as np\n",
    "npzfile = np.load('Question1p2.npz') # 'Question1p2.npz' is provided in github repo\n",
    "print(npzfile.files) # check the variable names\n",
    "x = npzfile['x']\n",
    "w = npzfile['w']\n",
    "b = npzfile['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.a) (1 point)\n",
    "stride=1, without zero padding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4x2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM LAB 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take shape from second part of question 1\n",
    "x_2d = torch.Tensor(x).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 1., 0., 0.],\n",
       "         [0., 1., 1., 1.],\n",
       "         [2., 2., 2., 0.],\n",
       "         [0., 2., 2., 2.]],\n",
       "\n",
       "        [[0., 0., 0., 1.],\n",
       "         [0., 1., 2., 2.],\n",
       "         [2., 0., 0., 1.],\n",
       "         [2., 2., 2., 0.]],\n",
       "\n",
       "        [[1., 0., 2., 0.],\n",
       "         [1., 2., 1., 2.],\n",
       "         [0., 2., 2., 0.],\n",
       "         [1., 1., 0., 0.]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_layer = nn.Conv2d(3,4,3,\n",
    "                         stride= 1, #Stride\n",
    "                         padding= 0, #Padding; \n",
    "                         bias= True\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv2d_layer(x_2d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b) (1 point) \n",
    "stride=2, padding = 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_layer = nn.Conv2d(3,4,3,\n",
    "                         stride= 2, #Stride\n",
    "                         padding= 2, #Padding; \n",
    "                         bias= True\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv2d_layer(x_2d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.c) (1 point)\n",
    "stride=3, padding = 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_layer = nn.Conv2d(3,4,3,\n",
    "                         stride= 3, #Stride\n",
    "                         padding= 2, #Padding; \n",
    "                         bias= True\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv2d_layer(x_2d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.d) (1 point) \n",
    "a dilated convolution with stride=1, dilation rate=2 and zero padding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_layer = nn.Conv2d(3,4,3,\n",
    "                         stride= 1, #Stride\n",
    "                         padding= 1, #Padding; \n",
    "                         bias= True,\n",
    "                         dilation = 2\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv2d_layer(x_2d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2, 2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape\n",
    "# come back to \n",
    "# https://piazza.com/class/k41ukp9bhhx55p?cid=43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.e) (1 point)\n",
    "What should be the padding (p) for zero padding when stride=2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_layer = nn.Conv2d(3,4,3,\n",
    "                         stride= 2, #Stride\n",
    "                         padding= 0, #Padding; In this case it's zero padding\n",
    "                         bias= True\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv2d_layer(x_2d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) (5 points)\n",
    "Use the pytorch package to calculate feature/activation maps. Write a code which takes 3x4x4 image and performs a 2D convolution operation (with stride = 1 and zero padding) using 3x3x3 filters provided on the picture. After convolution layer use leaky ReLU activation function (with negative slope 0.01) and Max-Pooling operation with required parameters to finally obtain output of dimension 4x1x1. Provide the code, feature maps obtained from convolution operation, activation maps, and feature maps after Max-Pooling operation.\n",
    "\n",
    "**Hint:** You can refer to [AdaptiveMaxPool2d](https://pytorch.org/docs/stable/nn.html#adaptivemaxpool2d) to get desired dimension output from Pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'w', 'b']\n"
     ]
    }
   ],
   "source": [
    "# starter code to load image:x, kernel weights:w and bias:b\n",
    "import numpy as np\n",
    "npzfile = np.load('Question1p2.npz') # 'Question1p2.npz' is provided in github repo\n",
    "print(npzfile.files) # check the variable names\n",
    "x = npzfile['x']\n",
    "w = npzfile['w']\n",
    "b = npzfile['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape #image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 3, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape #4 filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape #bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2d = torch.Tensor(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 1., 0., 0.],\n",
       "         [0., 1., 1., 1.],\n",
       "         [2., 2., 2., 0.],\n",
       "         [0., 2., 2., 2.]],\n",
       "\n",
       "        [[0., 0., 0., 1.],\n",
       "         [0., 1., 2., 2.],\n",
       "         [2., 0., 0., 1.],\n",
       "         [2., 2., 2., 0.]],\n",
       "\n",
       "        [[1., 0., 2., 0.],\n",
       "         [1., 2., 1., 2.],\n",
       "         [0., 2., 2., 0.],\n",
       "         [1., 1., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 3, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_2d = torch.nn.Parameter(torch.Tensor(w).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 1.,  2.,  2.],\n",
       "          [-2.,  2., -2.],\n",
       "          [ 1., -2.,  1.]],\n",
       "\n",
       "         [[ 2., -3., -2.],\n",
       "          [-3., -3.,  1.],\n",
       "          [ 3., -3.,  3.]],\n",
       "\n",
       "         [[ 1., -1.,  0.],\n",
       "          [-2., -1.,  2.],\n",
       "          [ 3., -1.,  2.]]],\n",
       "\n",
       "\n",
       "        [[[ 2.,  3.,  1.],\n",
       "          [ 0.,  2.,  3.],\n",
       "          [ 3.,  1.,  3.]],\n",
       "\n",
       "         [[-1., -1.,  1.],\n",
       "          [ 1., -1., -2.],\n",
       "          [ 0.,  3., -1.]],\n",
       "\n",
       "         [[-2.,  3.,  1.],\n",
       "          [-2., -3., -2.],\n",
       "          [-1., -2., -1.]]],\n",
       "\n",
       "\n",
       "        [[[-2., -1., -2.],\n",
       "          [-2.,  2.,  1.],\n",
       "          [-2., -1.,  1.]],\n",
       "\n",
       "         [[-3.,  2.,  1.],\n",
       "          [-2., -1., -2.],\n",
       "          [-2.,  3.,  0.]],\n",
       "\n",
       "         [[ 3.,  1.,  2.],\n",
       "          [-2.,  2.,  0.],\n",
       "          [-1., -3.,  1.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  1., -3.],\n",
       "          [ 3., -3.,  1.],\n",
       "          [-3.,  1., -2.]],\n",
       "\n",
       "         [[-3.,  3., -2.],\n",
       "          [-3., -1., -3.],\n",
       "          [ 0., -1., -1.]],\n",
       "\n",
       "         [[ 0.,  3., -1.],\n",
       "          [-2.,  1., -3.],\n",
       "          [-2., -2., -1.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_2d = torch.nn.Parameter(torch.Tensor(b).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.,  0.,  1.,  0.], requires_grad=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/custom-weight-initialization/20544/2\n",
    "class CNNet(nn.Module):\n",
    "    def __init__(self, w, b,  kernel_size = 3):\n",
    "        super(CNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,4, kernel_size=kernel_size)\n",
    "        with torch.no_grad():\n",
    "            self.conv1.weight = w\n",
    "            self.conv1.bias = b\n",
    "        #  After convolution layer use leaky ReLU activation function (with negative slope 0.01) \n",
    "        # and Max-Pooling operation with required parameters to finally obtain output of dimension 4x1x1\n",
    "        self.relu = nn.LeakyReLU() #default of neg .01 slope\n",
    "        self.pool = nn.AdaptiveMaxPool2d((1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(f'feature map after convolution operation with shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.pool(self.relu(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNet(w_2d,b_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map after convolution operation with shape: torch.Size([1, 4, 2, 2])\n",
      "tensor([[[[  9.,  -9.],\n",
      "          [ -7.,  -5.]],\n",
      "\n",
      "         [[  5.,  -1.],\n",
      "          [ 21.,  17.]],\n",
      "\n",
      "         [[ -9., -21.],\n",
      "          [  9.,   5.]],\n",
      "\n",
      "         [[-27., -28.],\n",
      "          [-16., -19.]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = cnn(x_2d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 1])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 9.0000]],\n",
       "\n",
       "         [[21.0000]],\n",
       "\n",
       "         [[ 9.0000]],\n",
       "\n",
       "         [[-0.1600]]]], grad_fn=<AdaptiveMaxPool2DBackward>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) (10 points)\n",
    "Use the pytorch package to calculate feature/activation maps of a residual unit as depicted in Figure 2 of https://arxiv.org/pdf/1512.03385.pdf as well as on the figure above.\n",
    "\n",
    "Input --> Convolution --> Activation --> Convolution --> Addition --> Activation\n",
    "\n",
    "Write a code which takes 3x4x4 input image and performs two 2D convolution operations using the filters provided on the figure. Please use first three 3x3x3 filters for the Convolution layers. After the convolution layers and after the residual addition use ReLU activation function. Provide the code and feature maps obtained from each convolution operation, activation maps, and the last activation map obtained from the residual unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.,  1., -3.],\n",
       "         [ 3., -3.,  1.],\n",
       "         [-3.,  1., -2.]],\n",
       "\n",
       "        [[-3.,  3., -2.],\n",
       "         [-3., -1., -3.],\n",
       "         [ 0., -1., -1.]],\n",
       "\n",
       "        [[ 0.,  3., -1.],\n",
       "         [-2.,  1., -3.],\n",
       "         [-2., -2., -1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Parameter(w_2d[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "relutest = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "relutest.weight = torch.nn.Parameter(w_2d[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 1., 0., 0.],\n",
       "         [0., 1., 1., 1.],\n",
       "         [2., 2., 2., 0.],\n",
       "         [0., 2., 2., 2.]],\n",
       "\n",
       "        [[0., 0., 0., 1.],\n",
       "         [0., 1., 2., 2.],\n",
       "         [2., 0., 0., 1.],\n",
       "         [2., 2., 2., 0.]],\n",
       "\n",
       "        [[1., 0., 2., 0.],\n",
       "         [1., 2., 1., 2.],\n",
       "         [0., 2., 2., 0.],\n",
       "         [1., 1., 0., 0.]]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/custom-weight-initialization/20544/2\n",
    "# https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278\n",
    "class CNNet(nn.Module):\n",
    "    def __init__(self, w, b,  kernel_size = 3):\n",
    "        super(CNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,12, kernel_size=kernel_size)\n",
    "        with torch.no_grad():\n",
    "            self.conv1.weight = torch.nn.Parameter(w[:3])\n",
    "            self.conv1.bias = torch.nn.Parameter(b[:3])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d((4,4))\n",
    "        self.conv2 = nn.Conv2d(3,3,kernel_size)\n",
    "#         with torch.no_grad():\n",
    "#             self.conv2.weight = torch.nn.Parameter(w[:3])\n",
    "#             self.conv2.bias = torch.nn.Parameter(b[:3])\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d((4,4))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        print(f'feature map after first convolution operation with shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.relu1(x)\n",
    "        print(f'feature map after first activation operation with shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.pool2(x)\n",
    "        print(f'feature map after pool operation with shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.conv2(x)\n",
    "        print(f'feature map after second convolution operation with shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.pool2(x)\n",
    "        print(f'feature map after pool operation with shape: {x.shape}')\n",
    "        print(x)\n",
    "        print(residual.shape)\n",
    "        x += residual\n",
    "        print(f'feature map after residual operation with shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.relu2(x)\n",
    "        print(f'feature map after second activation operation with shape: {x.shape}')\n",
    "        print(x)\n",
    "        x = self.avg(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNet(w_2d,b_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map after first convolution operation with shape: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[  9.,  -9.],\n",
      "          [ -7.,  -5.]],\n",
      "\n",
      "         [[  5.,  -1.],\n",
      "          [ 21.,  17.]],\n",
      "\n",
      "         [[ -9., -21.],\n",
      "          [  9.,   5.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "feature map after first activation operation with shape: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 9.,  0.],\n",
      "          [ 0.,  0.]],\n",
      "\n",
      "         [[ 5.,  0.],\n",
      "          [21., 17.]],\n",
      "\n",
      "         [[ 0.,  0.],\n",
      "          [ 9.,  5.]]]], grad_fn=<ReluBackward0>)\n",
      "feature map after pool operation with shape: torch.Size([1, 3, 4, 4])\n",
      "tensor([[[[ 9.,  9.,  0.,  0.],\n",
      "          [ 9.,  9.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.]],\n",
      "\n",
      "         [[ 5.,  5.,  0.,  0.],\n",
      "          [ 5.,  5.,  0.,  0.],\n",
      "          [21., 21., 17., 17.],\n",
      "          [21., 21., 17., 17.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.],\n",
      "          [ 9.,  9.,  5.,  5.],\n",
      "          [ 9.,  9.,  5.,  5.]]]], grad_fn=<AdaptiveMaxPool2DBackward>)\n",
      "feature map after second convolution operation with shape: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 5.1929,  4.3368],\n",
      "          [ 6.7562,  4.5635]],\n",
      "\n",
      "         [[ 4.4495,  4.8570],\n",
      "          [ 6.7284,  7.4276]],\n",
      "\n",
      "         [[-0.2023,  1.1989],\n",
      "          [ 4.5236,  5.2578]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "feature map after pool operation with shape: torch.Size([1, 3, 4, 4])\n",
      "tensor([[[[ 5.1929,  5.1929,  4.3368,  4.3368],\n",
      "          [ 5.1929,  5.1929,  4.3368,  4.3368],\n",
      "          [ 6.7562,  6.7562,  4.5635,  4.5635],\n",
      "          [ 6.7562,  6.7562,  4.5635,  4.5635]],\n",
      "\n",
      "         [[ 4.4495,  4.4495,  4.8570,  4.8570],\n",
      "          [ 4.4495,  4.4495,  4.8570,  4.8570],\n",
      "          [ 6.7284,  6.7284,  7.4276,  7.4276],\n",
      "          [ 6.7284,  6.7284,  7.4276,  7.4276]],\n",
      "\n",
      "         [[-0.2023, -0.2023,  1.1989,  1.1989],\n",
      "          [-0.2023, -0.2023,  1.1989,  1.1989],\n",
      "          [ 4.5236,  4.5236,  5.2578,  5.2578],\n",
      "          [ 4.5236,  4.5236,  5.2578,  5.2578]]]],\n",
      "       grad_fn=<AdaptiveMaxPool2DBackward>)\n",
      "torch.Size([1, 3, 4, 4])\n",
      "feature map after residual operation with shape: torch.Size([1, 3, 4, 4])\n",
      "tensor([[[[ 7.1929,  6.1929,  4.3368,  4.3368],\n",
      "          [ 5.1929,  6.1929,  5.3368,  5.3368],\n",
      "          [ 8.7562,  8.7562,  6.5635,  4.5635],\n",
      "          [ 6.7562,  8.7562,  6.5635,  6.5635]],\n",
      "\n",
      "         [[ 4.4495,  4.4495,  4.8570,  5.8570],\n",
      "          [ 4.4495,  5.4495,  6.8570,  6.8570],\n",
      "          [ 8.7284,  6.7284,  7.4276,  8.4276],\n",
      "          [ 8.7284,  8.7284,  9.4276,  7.4276]],\n",
      "\n",
      "         [[ 0.7977, -0.2023,  3.1989,  1.1989],\n",
      "          [ 0.7977,  1.7977,  2.1989,  3.1989],\n",
      "          [ 4.5236,  6.5236,  7.2578,  5.2578],\n",
      "          [ 5.5236,  5.5236,  5.2578,  5.2578]]]], grad_fn=<AddBackward0>)\n",
      "feature map after second activation operation with shape: torch.Size([1, 3, 4, 4])\n",
      "tensor([[[[7.1929, 6.1929, 4.3368, 4.3368],\n",
      "          [5.1929, 6.1929, 5.3368, 5.3368],\n",
      "          [8.7562, 8.7562, 6.5635, 4.5635],\n",
      "          [6.7562, 8.7562, 6.5635, 6.5635]],\n",
      "\n",
      "         [[4.4495, 4.4495, 4.8570, 5.8570],\n",
      "          [4.4495, 5.4495, 6.8570, 6.8570],\n",
      "          [8.7284, 6.7284, 7.4276, 8.4276],\n",
      "          [8.7284, 8.7284, 9.4276, 7.4276]],\n",
      "\n",
      "         [[0.7977, 0.0000, 3.1989, 1.1989],\n",
      "          [0.7977, 1.7977, 2.1989, 3.1989],\n",
      "          [4.5236, 6.5236, 7.2578, 5.2578],\n",
      "          [5.5236, 5.5236, 5.2578, 5.2578]]]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = cnn(x_2d.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[6.3374]],\n",
       "\n",
       "         [[6.8031]],\n",
       "\n",
       "         [[3.6446]]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 1])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://piazza.com/class/k41ukp9bhhx55p?cid=44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Network design parameters for disease classification (Total 15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disease classification is a common problem in medicine. There are many ways to solve this problem. Goal of this question is to make sure that you have a clear picture in your mind about possible techniques that you can use in such a classification task.\n",
    "\n",
    "Assume that we have a 10K images in a dataset of computed tomography (CTs). For each image, the dimension is 64x128x128 and we have the label for each image. The label of each image defines which class the image belongs (lets assume we have 4 different disease classes in total). You will describe your approach of classifying the disease for the techniques below. Make sure you do not forget the bias term. You can either design your proposed network by explaining it explicitely or you can provide the pytorch code which designs the network for questions 2.1.a, 2.2.a, and 2.3.a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code\n",
    "# you can generate a random image tensor for batch_size 8\n",
    "x = torch.Tensor(8,1,64,128,128).normal_().type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 64, 128, 128])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can explain the code or provide pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a) (2 points)\n",
    "Design a multi layer perceptron (MLP) with a two hidden layer which takes an image as input (by reshaping it to a vector: lets call this a vectorized image). Our network has to first map the vectorized images to a vector of 512, then to 128 in the second hidden layer and finally feeds this vector to a fully connected layer to get the probability of 4 tissue classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size=512, \n",
    "                 hidden_size2=128, out_features=1):\n",
    "        super(MLPNet,self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size2, out_features)\n",
    "        \n",
    "        # Define softmax output \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply relu and softmax \n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPNet(128*128*64, hidden_size=512, \n",
    "               hidden_size2=128, out_features=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b) (2 points)\n",
    "Clearly mention the sizes for your input and output at each layer until you get final output vector with 4 tissue classes in 64x128x128 voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*128*64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input size is 64 x 128 x 128\n",
    "output is size 4 because there are four classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.c) (1 points)\n",
    "How many parameters you need to fit for your design? How does adding another hidden layer effected the number of parameters to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPNet(\n",
       "  (fc1): Linear(in_features=1048576, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536871940"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536871940"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as seen in the previous homework, yes more hidden layers contribute to the count of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.a) (2 points)\n",
    "Design a one layer convolutional neural network which first maps the images to a vector of 256 (with the help of convolution and pooling operations) then feeds this vector to a fully connected layer to get the probability of 4 disease classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://piazza.com/class/k41ukp9bhhx55p?cid=101\n",
    "need a max pool 3d not 2d!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self, kernel_size = 3):\n",
    "        super(CNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(64,256, kernel_size=kernel_size, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.avg = nn.AdaptiveAvgPool3d(1)\n",
    "        self.linear = torch.nn.Linear(256, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(f'feature map after convolution operation with shape: {x.shape}')\n",
    "        x = self.relu1(x)\n",
    "        print(f'feature map after activation operation with shape: {x.shape}')\n",
    "        x = self.avg(x)\n",
    "        print(f'feature map after pooling operation with shape: {x.shape}')\n",
    "        x = self.linear(x.view(-1,256))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.b) (2 points)\n",
    "Clearly mention the sizes for your input, kernel, pooling, and output at each step until you get final output vector with 4 probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 128])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map after convolution operation with shape: torch.Size([1, 256, 63, 63])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 63, 63])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "out = cnn(x[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1599,  0.1219, -0.0473,  0.3623]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.c) (1 points) \n",
    "How many parameters you need to fit for your design?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_n_params(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNet(\n",
       "  (conv1): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (relu1): ReLU()\n",
       "  (avg): AdaptiveAvgPool2d(output_size=1)\n",
       "  (linear): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.d) (2 points)\n",
    "Now increase your selected convolution kernel size by 4 in each direction. Describe the effect of using small vs large filter size during convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNet(kernel_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 128])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map after convolution operation with shape: torch.Size([1, 256, 61, 61])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 61, 61])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "out = cnn(x[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804100"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_params(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before the parms were 148740 and now they have increased to 804100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a larger filter size increases at what seems like an exponential rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440.60777195105555"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100.0*(804100- 148740)/148740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133.33333333333334"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100.0*(7-3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594960"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "148740*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_parm_dict = {}\n",
    "def parm_size(kernel_size):\n",
    "    cnn = CNNet(kernel_size=kernel_size)\n",
    "    out = cnn(x[0][0].unsqueeze(0))\n",
    "    kernel_parm_dict[kernel_size] = get_n_params(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map after convolution operation with shape: torch.Size([1, 256, 64, 64])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 64, 64])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 63, 63])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 63, 63])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 63, 63])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 63, 63])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 62, 62])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 62, 62])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 62, 62])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 62, 62])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 61, 61])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 61, 61])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 61, 61])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 61, 61])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 60, 60])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 60, 60])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 60, 60])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 60, 60])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 59, 59])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 59, 59])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 59, 59])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 59, 59])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 58, 58])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 58, 58])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 58, 58])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 58, 58])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 57, 57])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 57, 57])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 57, 57])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 57, 57])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 56, 56])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 56, 56])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 56, 56])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 56, 56])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n",
      "feature map after convolution operation with shape: torch.Size([1, 256, 55, 55])\n",
      "feature map after activation operation with shape: torch.Size([1, 256, 55, 55])\n",
      "feature map after pooling operation with shape: torch.Size([1, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,20):\n",
    "    parm_size(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame.from_dict(kernel_parm_dict, orient='index', \n",
    "                       columns=['parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'kernel_size')"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp.plot(figsize= (8,5))\n",
    "plt.xlabel('kernel_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) (3 points)\n",
    "Explain your findings regading different types of neural networks and building blocks based on your observations from 2.1 and 2.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP yields many more parameters than the convolutional neural net. For the conv net with 3 kernels there were 148740 parameters and yet the MLP yielded 536871940. conv has less than .1 % of the MLP. Also the bigger the kernel for a convolutional network the parameters increases at an exponential rate.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002770493090028136"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "148740/536871940"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Literature Review: ChestX-ray8 (Total 15 points)\n",
    "Read this paper:\n",
    "\n",
    "Pranav Rajpurkar, Jeremy Irvin, et al. \n",
    "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning https://arxiv.org/abs/1711.05225\n",
    "\n",
    "\n",
    "We are interested in understanding the task, the methods that is proposed in this paper, technical aspects of the implementation, and possible future work. After you read the full article answer the following questions. Describe you answers in your own words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) (2 points) \n",
    "What type of learning algorithm is used (supervised, semi-supervised or unsupervised) for the classification? What is the reason for selecting this type of learning algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper uses the term \"weakly-supervised classification\". \n",
    "\n",
    "Overall, there is no existing database that has the full annotation because of the following\n",
    "1. Using Mechanical Turk at scale for these images does not make sense to put the annotation in the hands of non-medical professionals\n",
    "2. it is suspected that complete annotation of a specific area of the image would be needed. Right not detail to that extent isn't viable. \n",
    "3. The medical image diagnosis questions does not apply to the already available ImageNet pre-trained deep CNN models\n",
    "\n",
    "\n",
    "Since there is no database, they had to construct one with NLP and extrapolate some of the learnings and continue with supervised classification methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import excerts from the article: \n",
    "\n",
    "\"Deep neural network representations further make the joint language and vision\n",
    "learning tasks more feasible to solve, in image captioning\n",
    "[47, 23, 32, 46, 22], visual question answering [2, 45, 49, 53]\n",
    "and knowledge-guided transfer learning [4, 33], and so\n",
    "on. \"\n",
    "\n",
    "\n",
    "\". The main limitation is that all proposed methods are evaluated on some small-to-middle scale\n",
    "problems of (at most) several hundred patients. It remains\n",
    "unclear how well the current deep learning techniques will\n",
    "scale up to tens of thousands of patient studies.\"\n",
    "\n",
    "\n",
    "\" Region-level ImageNet pre-trained convolutional neural networks (CNN) based detectors are used\n",
    "to parse an input image and output a list of attributes or\n",
    "“visually-grounded high-level concepts”\"\n",
    "\n",
    "\"Fully dense annotation of\n",
    "region-level bounding boxes (for grounding the pathological findings) would normally be needed in computer vision\n",
    "datasets [32, 53, 24] but may be completely nonviable for\n",
    "the time being. Consequently, we formulate and verify a\n",
    "weakly-supervised multi-label image classification and disease localization framework to address this difficulty\"\n",
    "\n",
    "\"Deep Convolutional Neural\n",
    "Network (DCNN) architectures for weakly-supervised object localization, by considering large image capacity, various multi-label CNN losses and different pooling strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) (3 points)\n",
    "What type of convolutional neural network architectures were used in the paper? How does transfer learning from these architectures achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper leverages successful models previously built including: AlexNet, GoogLeNet, VGGNet, and ResNext\n",
    "\n",
    "The transistion layer is used after each of the pretrained models to get the data into one commmon dimension\n",
    "\n",
    "The paper mentions three different kinds of pooling used in the model:  max pooling, average pooling, and Log-Sum-Exp (LSE) pooling \n",
    "\n",
    "\n",
    "Transfer learning is used in the bounding boxs aka the annotations that are provided in detail on the images. instead of relying on those to be created they generated their own with a heat map and normalization and thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) (2 points)\n",
    "What is the loss function? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is calcluated over 8 dimensions, one for each classification. They use both a standard CEL (Cross entropy loss) and a weighted CEL. The weighted loss function was created by the group for this special case to accomodate the unbalanced cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) (3 points)\n",
    "\n",
    "Was the learning rate used changed during the training? Explain the situations where changing the learning rate is a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They likely changes the learning rate between the pretrained architectures,  as they used the following calcluation to limit GPU memory needed: batch size × iter size = 80 \n",
    "\n",
    "by this definition the iter_size would change depending upon the batch size and which architecture is used. \n",
    "\n",
    "\n",
    "\"it is necessary to reduce the image batch size to load\n",
    "the entire model and keep activations in GPU while we increase the iter size to accumulate the gradients for more iterations.\"\n",
    "\n",
    "\n",
    "\n",
    "Having a smaller learning rate may make the model more optimal, but also may make it slower to compute. A larger learning rate has the opposite effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5) (3 points)\n",
    "\n",
    "What are the evaluation metrics used for model comparison in classification? Explain why those metrics were chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They evaluate the AUCs of ROC curves, as well as accuracy and average false positives for each of these demiensions.\n",
    "\n",
    "AUC and accurary is a very common and helpful as it encompasses many aspects of evlauting classifcations (recall and sensitivity). Also the specific use case of this classificatiion task leads one to determine that the false posisitives is an import evaluation. This is because the use case is not diagnosing patients that aren't sick. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6) (2 points)\n",
    "Did the authors do any normalization to the data before feeding them to the model? If so, What's the purpose of doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization done on the model can be seen in the natural language processing of the radiology reports. \n",
    "\n",
    "\n",
    "On the images themselves the files are resized to 1024×1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Deep CNN design for disease classification (Total 45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the howework, we will focus on classifiying the lung disease using chest x-ray dataset provided by NIH (https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community). You should be familiar with the dataset after answering Question 3.\n",
    "\n",
    "You need to use HPC for training part of this question, as your computer's CPU will not be fast enough to compute learning iterations. In case you use HPC, please have your code/scripts uploaded under the questions and provide the required plots and tables there as well. Data is available in HPC under /beegfs/ga4493/data/HW2 folder. We are interested in classifying pneumothorax, cardiomegaly and infiltration cases. By saying so we have 3 classes that we want to identify by modelling a deep CNN.\n",
    "\n",
    "First, you need to work on Data_Entry_2017.csv file to identify cases/images that has infiltration, pneumothorax, and cardiomegaly. This file can be downloaded from https://nihcc.app.box.com/v/ChestXray-NIHCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Train, Test, and Validation Sets (2 points)\n",
    "Write a script to read data from Data_Entry_2017.csv and process to obtain 3 sets (train, validation and test). By using 'Finding Labels' column, define a class that each image belongs to, in total you can define 3 classes:\n",
    "- 0 cardiomegaly\n",
    "- 1 pneumothorax\n",
    "- 2 infiltration\n",
    "\n",
    "Generate a train, validation and test set by splitting the whole dataset containing specific classes (0, 1, and 2)  by 70%, 10% and 20%, respectively. Test set will not be used during modelling but it will be used to test your model's accuracy. Make sure you have similar percentages of different cases in each subset. Provide statistics of the number of classess in your subsets. (you do not need to think about splitting the sets based on subjects for this homework. In general, we do not want images from the same subject to appear in both train and test sets!!) \n",
    "\n",
    "Write a .csv files defining the samples in your train, validation and test set with names: train.csv, validation.csv, and test.csv. Submit these files with your homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "de2017 = pd.read_csv('Data_Entry_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00000003_001.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00000003_002.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2048</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00000003_003.png</td>\n",
       "      <td>Hernia|Infiltration</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2698</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00000003_004.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00000003_005.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2686</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0  00000001_000.png            Cardiomegaly            0           1   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "3  00000002_000.png              No Finding            0           2   \n",
       "4  00000003_000.png                  Hernia            0           3   \n",
       "5  00000003_001.png                  Hernia            1           3   \n",
       "6  00000003_002.png                  Hernia            2           3   \n",
       "7  00000003_003.png     Hernia|Infiltration            3           3   \n",
       "8  00000003_004.png                  Hernia            4           3   \n",
       "9  00000003_005.png                  Hernia            5           3   \n",
       "\n",
       "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0           58              M            PA                 2682     2749   \n",
       "1           58              M            PA                 2894     2729   \n",
       "2           58              M            PA                 2500     2048   \n",
       "3           81              M            PA                 2500     2048   \n",
       "4           81              F            PA                 2582     2991   \n",
       "5           74              F            PA                 2500     2048   \n",
       "6           75              F            PA                 2048     2500   \n",
       "7           76              F            PA                 2698     2991   \n",
       "8           77              F            PA                 2500     2048   \n",
       "9           78              F            PA                 2686     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
       "0                        0.143  0.143          NaN  \n",
       "1                        0.143  0.143          NaN  \n",
       "2                        0.168  0.168          NaN  \n",
       "3                        0.171  0.171          NaN  \n",
       "4                        0.143  0.143          NaN  \n",
       "5                        0.168  0.168          NaN  \n",
       "6                        0.168  0.168          NaN  \n",
       "7                        0.143  0.143          NaN  \n",
       "8                        0.168  0.168          NaN  \n",
       "9                        0.143  0.143          NaN  "
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2017.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['cardiomegaly', 'pneumothorax', 'infiltration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[0] in de2017['Finding Labels'][1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define classes\n",
    "for class_name in classes:\n",
    "    de2017[class_name] = de2017['Finding Labels'].apply(lambda x: 1 if class_name in x.lower() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "de2017['multicheck'] = de2017[classes].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "de2017 = de2017[de2017['multicheck'] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>cardiomegaly</th>\n",
       "      <th>pneumothorax</th>\n",
       "      <th>infiltration</th>\n",
       "      <th>multicheck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Image Index, Finding Labels, Follow-up #, Patient ID, Patient Age, Patient Gender, View Position, OriginalImage[Width, Height], OriginalImagePixelSpacing[x, y], Unnamed: 11, cardiomegaly, pneumothorax, infiltration, multicheck]\n",
       "Index: []"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2017.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "de2017.drop('Unnamed: 11', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112120, 1)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110570, 15)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>cardiomegaly</th>\n",
       "      <th>pneumothorax</th>\n",
       "      <th>infiltration</th>\n",
       "      <th>multicheck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0  00000001_000.png            Cardiomegaly            0           1   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "3  00000002_000.png              No Finding            0           2   \n",
       "4  00000003_000.png                  Hernia            0           3   \n",
       "\n",
       "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0           58              M            PA                 2682     2749   \n",
       "1           58              M            PA                 2894     2729   \n",
       "2           58              M            PA                 2500     2048   \n",
       "3           81              M            PA                 2500     2048   \n",
       "4           81              F            PA                 2582     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  cardiomegaly  pneumothorax  \\\n",
       "0                        0.143  0.143             1             0   \n",
       "1                        0.143  0.143             1             0   \n",
       "2                        0.168  0.168             1             0   \n",
       "3                        0.171  0.171             0             0   \n",
       "4                        0.143  0.143             0             0   \n",
       "\n",
       "   infiltration  multicheck  \n",
       "0             0           1  \n",
       "1             0           1  \n",
       "2             0           1  \n",
       "3             0           0  \n",
       "4             0           0  "
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sfalk/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "de2017['Class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>cardiomegaly</th>\n",
       "      <th>pneumothorax</th>\n",
       "      <th>infiltration</th>\n",
       "      <th>multicheck</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0  00000001_000.png            Cardiomegaly            0           1   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "3  00000002_000.png              No Finding            0           2   \n",
       "4  00000003_000.png                  Hernia            0           3   \n",
       "\n",
       "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0           58              M            PA                 2682     2749   \n",
       "1           58              M            PA                 2894     2729   \n",
       "2           58              M            PA                 2500     2048   \n",
       "3           81              M            PA                 2500     2048   \n",
       "4           81              F            PA                 2582     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  cardiomegaly  pneumothorax  \\\n",
       "0                        0.143  0.143             1             0   \n",
       "1                        0.143  0.143             1             0   \n",
       "2                        0.168  0.168             1             0   \n",
       "3                        0.171  0.171             0             0   \n",
       "4                        0.143  0.143             0             0   \n",
       "\n",
       "   infiltration  multicheck  Class  \n",
       "0             0           1      0  \n",
       "1             0           1      0  \n",
       "2             0           1      0  \n",
       "3             0           0      0  \n",
       "4             0           0      0  "
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'cardiomegaly':0, 'pneumothorax':1, 'infiltration':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19226488/change-one-value-based-on-another-value-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sfalk/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "for k,v in class_dict.items():\n",
    "    de2017.loc[de2017[k] == 1, 'Class'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove remaining records\n",
    "de2017 = de2017[de2017['multicheck'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24856, 16)"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = de2017.iloc[:,:11::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['class'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24856, 12)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00000003_003.png</td>\n",
       "      <td>Hernia|Infiltration</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>76</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2698</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00000005_006.png</td>\n",
       "      <td>Infiltration</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2992</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0   00000001_000.png            Cardiomegaly            0           1   \n",
       "1   00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2   00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "7   00000003_003.png     Hernia|Infiltration            3           3   \n",
       "19  00000005_006.png            Infiltration            6           5   \n",
       "\n",
       "    Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0            58              M            PA                 2682     2749   \n",
       "1            58              M            PA                 2894     2729   \n",
       "2            58              M            PA                 2500     2048   \n",
       "7            76              F            PA                 2698     2991   \n",
       "19           70              F            PA                 2992     2991   \n",
       "\n",
       "    OriginalImagePixelSpacing[x     y]  class  \n",
       "0                         0.143  0.143      0  \n",
       "1                         0.143  0.143      0  \n",
       "2                         0.168  0.168      0  \n",
       "7                         0.143  0.143      2  \n",
       "19                        0.143  0.143      2  "
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = de2017.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/8, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17398, 12)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17398,)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4972, 12)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4972,)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2486, 12)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2486,)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    12830\n",
       "1     3069\n",
       "0     1499\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.737441\n",
       "1    0.176400\n",
       "0    0.086159\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.754224\n",
       "1    0.158488\n",
       "0    0.087289\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1875\n",
       "1     394\n",
       "0     217\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3672\n",
       "1     860\n",
       "0     440\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.738536\n",
       "1    0.172969\n",
       "0    0.088496\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even percent of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('train.csv', index=False)\n",
    "X_val.to_csv('validation.csv', index=False)\n",
    "X_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Data preparation before training\n",
    "From here on, you will use HW2_trainSet.csv, HW2_testSet.csv and HW2_validationSet.csv provided under github repo for defining train, test and validation set samples instead of the csv files you generate on Question 4.1.\n",
    "\n",
    "\n",
    "There are multiple ways of using images as an input during training or validation. Here, you will use torch Dataset class  (http://pytorch.org/tutorials/beginner/data_loading_tutorial.html). We provided the dataloader code below. Please explain what does following lines of code achieves in the data loader and propose alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('HW2_testSet.csv')\n",
    "df_train = pd.read_csv('HW2_trainSet.csv')\n",
    "df_val = pd.read_csv('HW2_validationSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00015420_003.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>3</td>\n",
       "      <td>15420</td>\n",
       "      <td>45</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2992</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016587_005.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>5</td>\n",
       "      <td>16587</td>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2568</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00029909_000.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>0</td>\n",
       "      <td>29909</td>\n",
       "      <td>73</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00012288_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>12288</td>\n",
       "      <td>65</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2990</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00026278_001.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>1</td>\n",
       "      <td>26278</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2402</td>\n",
       "      <td>2489</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index Finding Labels  Follow-up #  Patient ID  Patient Age  \\\n",
       "0  00015420_003.png   Pneumothorax            3       15420           45   \n",
       "1  00016587_005.png   Pneumothorax            5       16587           48   \n",
       "2  00029909_000.png   Pneumothorax            0       29909           73   \n",
       "3  00012288_000.png   Cardiomegaly            0       12288           65   \n",
       "4  00026278_001.png   Pneumothorax            1       26278           50   \n",
       "\n",
       "  Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0              M            PA                 2992     2991   \n",
       "1              F            PA                 2568     2991   \n",
       "2              M            AP                 3056     2544   \n",
       "3              F            PA                 2990     2991   \n",
       "4              F            PA                 2402     2489   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Class  \n",
       "0                        0.143  0.143      1  \n",
       "1                        0.143  0.143      1  \n",
       "2                        0.139  0.139      1  \n",
       "3                        0.143  0.143      0  \n",
       "4                        0.143  0.143      1  "
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00028871_001.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>1</td>\n",
       "      <td>28871</td>\n",
       "      <td>42</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00028208_023.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>23</td>\n",
       "      <td>28208</td>\n",
       "      <td>64</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001583_005.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>5</td>\n",
       "      <td>1583</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2602</td>\n",
       "      <td>2557</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00018921_060.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>60</td>\n",
       "      <td>18921</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2576</td>\n",
       "      <td>2519</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00023313_009.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>9</td>\n",
       "      <td>23313</td>\n",
       "      <td>50</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2992</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index Finding Labels  Follow-up #  Patient ID  Patient Age  \\\n",
       "0  00028871_001.png   Cardiomegaly            1       28871           42   \n",
       "1  00028208_023.png   Pneumothorax           23       28208           64   \n",
       "2  00001583_005.png   Pneumothorax            5        1583           16   \n",
       "3  00018921_060.png   Pneumothorax           60       18921           42   \n",
       "4  00023313_009.png   Pneumothorax            9       23313           50   \n",
       "\n",
       "  Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0              M            AP                 3056     2544   \n",
       "1              M            AP                 3056     2544   \n",
       "2              M            PA                 2602     2557   \n",
       "3              F            PA                 2576     2519   \n",
       "4              M            PA                 2992     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Class  \n",
       "0                        0.139  0.139      0  \n",
       "1                        0.139  0.139      1  \n",
       "2                        0.143  0.143      1  \n",
       "3                        0.143  0.143      1  \n",
       "4                        0.143  0.143      1  "
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005038_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>5038</td>\n",
       "      <td>12</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2192</td>\n",
       "      <td>2194</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00010967_001.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>1</td>\n",
       "      <td>10967</td>\n",
       "      <td>69</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2802</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00006624_003.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>3</td>\n",
       "      <td>6624</td>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2442</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005365_008.png</td>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>8</td>\n",
       "      <td>5365</td>\n",
       "      <td>13</td>\n",
       "      <td>F</td>\n",
       "      <td>AP</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00013062_003.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>3</td>\n",
       "      <td>13062</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2992</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index Finding Labels  Follow-up #  Patient ID  Patient Age  \\\n",
       "0  00005038_000.png   Cardiomegaly            0        5038           12   \n",
       "1  00010967_001.png   Pneumothorax            1       10967           69   \n",
       "2  00006624_003.png   Cardiomegaly            3        6624           56   \n",
       "3  00005365_008.png   Pneumothorax            8        5365           13   \n",
       "4  00013062_003.png   Cardiomegaly            3       13062           60   \n",
       "\n",
       "  Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0              F            PA                 2192     2194   \n",
       "1              M            PA                 2802     2991   \n",
       "2              F            PA                 2442     2991   \n",
       "3              F            AP                 2500     2048   \n",
       "4              F            PA                 2992     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Class  \n",
       "0                        0.143  0.143      0  \n",
       "1                        0.143  0.143      1  \n",
       "2                        0.143  0.143      0  \n",
       "3                        0.171  0.171      1  \n",
       "4                        0.143  0.143      0  "
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.a) (2 points) \n",
    "\n",
    "image = io.imread(img_name,as_gray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in the image to object \"image\" from path img_name. Use grayscale for the image. \n",
    "\n",
    "Possible to read in as RGB, but not applicable here\n",
    "\n",
    "You can also read an entire collection in at once w/ imread_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.b) (2 points) \n",
    "\n",
    "image = (image - image.mean()) / image.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize the image with standard normalization procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using equalize could be an efficient way to normalize as well. Could occur globally or locally\n",
    "\n",
    "Global equalize \n",
    "img_rescale = exposure.equalize_hist(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io\n",
    "import torch\n",
    "from skimage import color\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"Chest X-ray dataset from https://nihcc.app.box.com/v/ChestXray-NIHCC.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data_frame.iloc[idx, 0])\n",
    "\n",
    "        image = io.imread(img_name,as_gray=True)\n",
    "        \n",
    "        image = (image - image.mean()) / image.std()\n",
    "            \n",
    "        image_class = self.data_frame.iloc[idx, -1]\n",
    "\n",
    "        sample = {'x': image[None,:], 'y': image_class}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) CNN model definition (6 points)\n",
    "Since now we can import images for model training, next step is to define a CNN model that you will use to train disease classification task. Any model requires us to select model parameters like how many layers, what is the kernel size, how many feature maps and so on. The number of possible models is infinite, but we need to make some design choices to start.  Lets design a CNN model with 3 convolutional layers, 3 residual units (similar to Figure 2 of https://arxiv.org/pdf/1512.03385.pdf) and a fully connected (FC) layer followed by a classification layer. Lets use \n",
    "\n",
    "-  3x3 convolution kernels (stride 1 in resnet units and stride 2 in convolutional layers)\n",
    "-  ReLU for an activation function\n",
    "-  max pooling with kernel 2x2 and stride 2 only after the convolutional layers. \n",
    "\n",
    "Define the number of feature maps in hidden layers as: 16, 16, 16, 32, 32, 32, 64, 64, 64, 128 (1st layer, ..., 10th layer). \n",
    "\n",
    "Input --> Convolution1 --> ResNetBlock1 --> Convolution2 --> ResNetBlock2 --> Convolution3 --> ResNetBlock3 --> FC --> Classification Layer\n",
    "\n",
    "\n",
    "Write a class which specifies this network details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_model(nn.Module):\n",
    "    def __init__(self, kernel_size = 3):\n",
    "        super(Conv_model,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size, stride = 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16,32,kernel_size, stride = 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32,64,kernel_size, padding = 1, stride = 3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64,128,kernel_size, padding = 1, stride = 3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128,256,kernel_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.avg = nn.AdaptiveAvgPool2d(2,2)\n",
    "        self.linear = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.relu5(self.conv5(x))\n",
    "        x = self.avg(x)\n",
    "        x = self.linear(x.view(-1,256))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4) (2 points)\n",
    "How many learnable parameters of this model has? How many learnable parameters we would have if we replace the fully connected layer with global average pooling layer (Take a look at Section 3.2 of https://arxiv.org/pdf/1312.4400.pdf)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5) Loss function and optimizer (2 points)\n",
    "Define an appropriate loss criterion and an optimizer using pytorch. What type of loss function is applicable to our classification problem? Explain your choice of a loss function.  For an optimizer lets use Adam for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Some background:_ In network architecture design, we want to have an architecture that has enough capacity to learn. We can achieve this by using large number of feature maps and/or many more connections and activation nodes. However, having a large number of learnable parameters can easily result in overfitting. To mitigate overfitting, we can keep the number of learnable parameters of the network small either using shallow networks or few feature maps. This approach results in underfitting that model can neither model the training data nor generalize to new data. Ideally, we want to select a model at the sweet spot between underfitting and overfitting. It is hard to find the exact sweet spot. \n",
    "\n",
    "We first need to make sure we have enough capacity to learn, without a capacity we will underfit. Here, you will need to check if designed model in 4.3. can learn or not. Since we do not need to check the generalization capacity (overfitting is OK for now since it shows learning is possible), it is a great strategy to use a subset of training samples. Also, using a subset of samples is helpful for debugging!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6) Train the network on a subset (8 points)\n",
    "Lets use a script to take random samples from train set (HW2_trainSet.csv), lets name this set as HW2_randomTrainSet. Choose random samples from validation set (HW2_validationSet.csv), lets name this set as HW2_randomValidationSet. You used downsampling of images from 1024x1024 size to 64x64 in the Lab 4. This was fine for learning purpose but it will significantly reduce the infomation content of the images which is important especially in medicine. In this Homework, you MUST use original images of size 1024x1024 as the network input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get samples from HW2_trainSet.csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('HW2_trainSet.csv')\n",
    "_ , X_random, _, _ = train_test_split(df, df.Class, test_size=0.1, random_state=0)\n",
    "print('Selected subset class frequencies\\n',X_random['Class'].value_counts())\n",
    "X_random.to_csv('HW2_randomTrainSet.csv',index=False)\n",
    "\n",
    "df = pd.read_csv('HW2_validationSet.csv')\n",
    "_ , X_random, _, _ = train_test_split(df, df.Class, test_size=0.1, random_state=0)\n",
    "print('Selected subset class frequencies\\n',X_random['Class'].value_counts())\n",
    "X_random.to_csv('HW2_randomValidationSet.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the random samples generated and write a script to train your network. Using the script train your network using your choice of weight initialization strategy. In case you need to define other hyperparameters choose them empirically, for example batch size. Plot average loss on your random sample set per epoch. (Stop the training after at most ~50 epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7) Analysis of training using a CNN model(2 points)\n",
    "Describe your findings. Can your network learn from small subset of random samples? Does CNN model have enough capacity to learn with your choice of emprical hyperparameters?\n",
    "-  If yes, how will average loss plot will change if you multiply the learning rate by 10?\n",
    "-  If no, how can you increase the model capacity? Increase your model capacity and train again until you find a model with enough capacity. If the capacity increase is not sufficient to learn, think about empirical parameters you choose in designing your network and make some changes on your selection. Describe what type of changes you made to your original network and how can you manage this model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8) Train the network on the whole dataset (6 points)\n",
    "After question 4.7., you should have a network which has enough capacity to learn and you were able to debug your training code so that it is now ready to be trained on the whole dataset. Train your network on the whole train set (HW2_trainSet.csv) and check the validation loss on the whole validation set (HW2_validationSet.csv) in each epoch. Plot average loss and accuracy on train and validation sets. Describe your findings. Do you see overfitting or underfitting to train set? What else you can do to mitigate it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9) Experiments with Resnet18 (13 Points)\n",
    "\n",
    "Let's use Resnet18 on our dataset and see how it performs. We can import the standard architectures directly using PyTorch's torchvison.models module. Refer to https://pytorch.org/docs/stable/torchvision/models.html to see all available models in PyTorch. You'll later, in this course, learn about a convenient and useful concept known as Transfer Learning. For now, we will  use the Resnet18 and train the architecture from scratch without any pre-training. Here is the link for the ResNet paper: https://arxiv.org/pdf/1512.03385.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.1) (3 Points)\n",
    "What is the reason of using 1x1 convolutions before 3x3 convolutions in the resnet architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2) Train the ResNet18 on the whole dataset (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a new dataset class and a few additional transformations to the data for this new architecture. We have a new dataset class as ResNet18 architectures expect 3 channels in their primary input and other reasons which you'll later come to know - after the lecture on transfer learning. Nevertheless, for our case, we use them to reduce the required GPU usage as the Resnet18 architecture is significantly complex and GPU memory-intensive architecture than the CNN implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# torchvision models are trained on input images normalized to [0 1] range .ToPILImage() function achives this\n",
    "# additional normalization is required see: http://pytorch.org/docs/master/torchvision/models.html\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(896),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop(896),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "class ChestXrayDataset_ResNet(Dataset):\n",
    "    \"\"\"Chest X-ray dataset from https://nihcc.app.box.com/v/ChestXray-NIHCC.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data_frame.iloc[idx, 0])\n",
    "        \n",
    "        image = io.imread(img_name)\n",
    "        if len(image.shape) > 2 and image.shape[2] == 4:\n",
    "            image = image[:,:,0]\n",
    "            \n",
    "        image=np.repeat(image[None,...],3,axis=0)\n",
    "            \n",
    "        image_class = self.data_frame.iloc[idx, -1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {'x': image, 'y': image_class}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2.a) Architecture modification (6 points) \n",
    "In this question you need to develop a CNN model based on Resnet18 architecture. Please import the original ResNet18 model from PyTorch models (You can also implement this model by your own using the resnet paper). Modify the architecture so that the model will work with full size 1024x1024 image inputs and 3 classes of our interest:\n",
    "- 0 cardiomegaly\n",
    "- 1 pneumothorax\n",
    "- 2 infiltration\n",
    "\n",
    "Make sure the model you developed uses random weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2.b)Train the network on the whole dataset (4 points)\n",
    "Similar to section 4.7. train the model you developed in section 4.9.2.a on the whole train set (HW2_trainSet.csv) and check the validation loss on the whole validation set (HW2_validationSet.csv) in each epoch. Plot average loss and accuracy on train and validation sets. Describe your findings. Do you see overfitting or underfitting to train set? What else you can do to mitigate it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Analysis of the results from two networks trained on the full dataset (Total 5 points + Bonus Question Maximum 5 points)\n",
    "Use the validation loss to choose models from Q4.8 (model1) and Q4.9 (model2) (These models are trained on the full dataset and they learned from train data and generalized well to the validation set). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1) Model selection by performance on test set (5 Points)\n",
    "Using these models, plot confusion matrix and ROC curve for the disease classifier on the test set (HW2_TestSet.csv). Report AUC for this CNN model as the performance metric. You will have two confusion matrices and two ROC curves to compare model1 and model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the place we predict the disease from a model trained, output for this function is \n",
    "#the target values and probabilty of each image having a disease \n",
    "\n",
    "# Example of how to plot ROC curves\n",
    "# https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "\n",
    "# Example of how to calculate confusion matrix\n",
    "# https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Understanding the network ( Bonus Question maximum 5 points)\n",
    "\n",
    "Using the best performing model (choose the model using the analysis you performed on Q5.1), we will figure out where our network gathers infomation to decide the class for the image. One way of doing this is to occlude parts of the image and run through your network. By changing the location of the ocluded region we can visualize the probability of image being in one class as a 2-dimensional heat map. Using the best performing model, provide the heat map of the following images: HW2_visualize.csv. Do the heap map and bounding box for pathologies provide similar information? Describe your findings.\n",
    "Reference: https://arxiv.org/pdf/1311.2901.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the code from: https://github.com/thesemicolonguy/convisualize_nb/blob/master/cnn-visualize.ipynb \n",
    "# with minor modifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
